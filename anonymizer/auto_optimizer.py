"""
ìë™ í•˜ë“œì›¨ì–´ ìµœì í™” ì‹œìŠ¤í…œ
ì»´í“¨í„° í™˜ê²½ì— ë”°ë¼ ìµœì ì˜ ë°°ì¹˜ í¬ê¸°ì™€ ì„¤ì •ê°’ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ ì„¤ì •
"""
from __future__ import annotations
import os
import time
import psutil
import torch
import cv2
import numpy as np
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
import json
from pathlib import Path

@dataclass
class HardwareInfo:
    """í•˜ë“œì›¨ì–´ ì •ë³´ í´ë˜ìŠ¤"""
    cpu_cores: int
    cpu_threads: int
    total_ram_gb: float
    available_ram_gb: float
    gpu_available: bool
    gpu_name: str = ""
    gpu_memory_gb: float = 0.0
    gpu_compute_capability: Tuple[int, int] = (0, 0)

@dataclass
class OptimalSettings:
    """ìµœì í™”ëœ ì„¤ì •ê°’ í´ë˜ìŠ¤"""
    batch_size: int
    cpu_workers: int
    queue_size: int
    confidence: float
    pose_model: str
    eye_detection_interval: int
    half_precision: bool
    opencv_threads: int

class HardwareProfiler:
    """í•˜ë“œì›¨ì–´ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„"""
    
    @staticmethod
    def detect_hardware() -> HardwareInfo:
        """ì‹œìŠ¤í…œ í•˜ë“œì›¨ì–´ ì •ë³´ ìë™ ê°ì§€"""
        # CPU ì •ë³´
        cpu_cores = psutil.cpu_count(logical=False)
        cpu_threads = psutil.cpu_count(logical=True)
        
        # ë©”ëª¨ë¦¬ ì •ë³´
        memory = psutil.virtual_memory()
        total_ram_gb = memory.total / (1024**3)
        available_ram_gb = memory.available / (1024**3)
        
        # GPU ì •ë³´
        gpu_available = torch.cuda.is_available()
        gpu_name = ""
        gpu_memory_gb = 0.0
        gpu_compute_capability = (0, 0)
        
        if gpu_available:
            try:
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                gpu_compute_capability = torch.cuda.get_device_capability(0)
                print(f"[GPU] ê°ì§€ë¨: {gpu_name} ({gpu_memory_gb:.1f}GB)")
            except Exception as e:
                print(f"[GPU] ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                gpu_available = False
        
        return HardwareInfo(
            cpu_cores=cpu_cores,
            cpu_threads=cpu_threads,
            total_ram_gb=total_ram_gb,
            available_ram_gb=available_ram_gb,
            gpu_available=gpu_available,
            gpu_name=gpu_name,
            gpu_memory_gb=gpu_memory_gb,
            gpu_compute_capability=gpu_compute_capability
        )
    
    @staticmethod
    def estimate_memory_usage(batch_size: int, image_size: Tuple[int, int] = (640, 640)) -> float:
        """ë°°ì¹˜ í¬ê¸°ë³„ ì˜ˆìƒ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚° (GB)"""
        h, w = image_size
        # YOLOv8 ëª¨ë¸ + ì…ë ¥ ì´ë¯¸ì§€ + ì¤‘ê°„ ê²°ê³¼ë¬¼ ì˜ˆìƒ
        model_memory = 0.5  # ê¸°ë³¸ ëª¨ë¸ ë©”ëª¨ë¦¬ (GB)
        input_memory = batch_size * h * w * 3 * 4 / (1024**3)  # float32 ê¸°ì¤€
        intermediate_memory = input_memory * 3  # ì¤‘ê°„ ì²˜ë¦¬ ê²°ê³¼
        
        return model_memory + input_memory + intermediate_memory

class AutoTuner:
    """ìë™ ì„¤ì • ìµœì í™”"""
    
    def __init__(self, hardware_info: HardwareInfo):
        self.hw = hardware_info
    
    def calculate_optimal_batch_size(self) -> int:
        """GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if not self.hw.gpu_available:
            return 1
        
        # GPU ë©”ëª¨ë¦¬ì˜ 80%ë¥¼ ì‚¬ìš© ëª©í‘œ
        target_memory = self.hw.gpu_memory_gb * 0.8
        
        # ì´ì§„ íƒìƒ‰ìœ¼ë¡œ ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
        min_batch, max_batch = 1, 64
        optimal_batch = 1
        
        for batch_size in [1, 2, 4, 8, 16, 32, 48, 64]:
            estimated_usage = HardwareProfiler.estimate_memory_usage(batch_size)
            if estimated_usage <= target_memory:
                optimal_batch = batch_size
            else:
                break
        
        print(f"[AutoTuner] ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch} (ì˜ˆìƒ ë©”ëª¨ë¦¬: {HardwareProfiler.estimate_memory_usage(optimal_batch):.1f}GB)")
        return optimal_batch
    
    def calculate_cpu_workers(self) -> int:
        """CPU ì½”ì–´ ê¸°ë°˜ ìµœì  ì›Œì»¤ ìˆ˜ ê³„ì‚°"""
        # CPU ì½”ì–´ì˜ 75% ì‚¬ìš© (ì‹œìŠ¤í…œ ì˜ˆì•½ ê³ ë ¤)
        optimal_workers = max(1, int(self.hw.cpu_cores * 0.75))
        return min(optimal_workers, 8)  # ìµœëŒ€ 8ê°œë¡œ ì œí•œ
    
    def calculate_queue_sizes(self) -> int:
        """RAM ê¸°ë°˜ ìµœì  í í¬ê¸° ê³„ì‚°"""
        # ì‚¬ìš© ê°€ëŠ¥í•œ RAMì˜ 10% ì‚¬ìš©
        available_memory_mb = self.hw.available_ram_gb * 1024 * 0.1
        
        # í”„ë ˆì„ë‹¹ ëŒ€ëµ 2MB ê°€ì • (1080p ê¸°ì¤€)
        frame_size_mb = 2
        queue_size = max(8, int(available_memory_mb / frame_size_mb / 4))  # 4ê°œ íë¡œ ë¶„ì‚°
        
        return min(queue_size, 64)  # ìµœëŒ€ 64ë¡œ ì œí•œ
    
    def select_optimal_model(self) -> str:
        """GPU ì„±ëŠ¥ì— ë”°ë¥¸ ìµœì  ëª¨ë¸ ì„ íƒ"""
        if not self.hw.gpu_available:
            return "models/yolov8n-pose.pt"
        
        # GPU ë©”ëª¨ë¦¬ì™€ ì»´í“¨íŠ¸ ëŠ¥ë ¥ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ
        if self.hw.gpu_memory_gb >= 8 and self.hw.gpu_compute_capability[0] >= 7:
            return "models/yolov8m-pose.pt"  # ì¤‘ê°„ ëª¨ë¸
        elif self.hw.gpu_memory_gb >= 4:
            return "models/yolov8s-pose.pt"  # ì‘ì€ ëª¨ë¸
        else:
            return "models/yolov8n-pose.pt"  # ë‚˜ë…¸ ëª¨ë¸
    
    def should_use_half_precision(self) -> bool:
        """Half precision ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        if not self.hw.gpu_available:
            return False
        
        # Compute Capability 7.0 ì´ìƒì—ì„œ half precision ì§€ì›
        return self.hw.gpu_compute_capability[0] >= 7

class RuntimeOptimizer:
    """ëŸ°íƒ€ì„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë™ì  ì¡°ì •"""
    
    def __init__(self):
        self.performance_history = []
        self.oom_count = 0
        self.last_adjustment_time = 0
        
    def monitor_gpu_memory(self) -> Tuple[float, float]:
        """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
        if not torch.cuda.is_available():
            return 0.0, 0.0
        
        try:
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            cached = torch.cuda.memory_reserved(0) / (1024**3)
            return allocated, cached
        except:
            return 0.0, 0.0
    
    def should_reduce_batch_size(self, current_fps: float, target_fps: float = 30.0) -> bool:
        """ë°°ì¹˜ í¬ê¸° ê°ì†Œ ì—¬ë¶€ íŒë‹¨"""
        # OOMì´ ë°œìƒí–ˆê±°ë‚˜ ì„±ëŠ¥ì´ ë„ˆë¬´ ë‚®ì€ ê²½ìš°
        if self.oom_count > 0:
            return True
        
        # ìµœê·¼ ì„±ëŠ¥ì´ ëª©í‘œ FPSì˜ 50% ë¯¸ë§Œì¸ ê²½ìš°
        if len(self.performance_history) >= 5:
            recent_avg = sum(self.performance_history[-5:]) / 5
            if recent_avg < target_fps * 0.5:
                return True
        
        return False
    
    def record_performance(self, fps: float, memory_usage: float):
        """ì„±ëŠ¥ ê¸°ë¡"""
        self.performance_history.append(fps)
        
        # ìµœê·¼ 20ê°œ ê¸°ë¡ë§Œ ìœ ì§€
        if len(self.performance_history) > 20:
            self.performance_history.pop(0)
    
    def handle_oom_error(self):
        """OOM ì—ëŸ¬ ì²˜ë¦¬"""
        self.oom_count += 1
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print(f"[RuntimeOptimizer] OOM ë°œìƒ (ì´ {self.oom_count}íšŒ)")

class AutoConfig:
    """ìë™ ìµœì í™” ì„¤ì • ìƒì„±ê¸°"""
    
    def __init__(self, cache_file: str = "auto_config_cache.json"):
        self.cache_file = Path(cache_file)
        self.hardware_info = None
        self.optimal_settings = None
        self.runtime_optimizer = RuntimeOptimizer()
    
    def load_or_detect_hardware(self) -> HardwareInfo:
        """í•˜ë“œì›¨ì–´ ì •ë³´ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ê°ì§€"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r') as f:
                    cached_data = json.load(f)
                    
                # ìºì‹œëœ í•˜ë“œì›¨ì–´ ì •ë³´ì™€ í˜„ì¬ ì •ë³´ ë¹„êµ
                current_hw = HardwareProfiler.detect_hardware()
                cached_hw = HardwareInfo(**cached_data.get('hardware_info', {}))
                
                # ì£¼ìš” í•˜ë“œì›¨ì–´ê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìºì‹œ ì‚¬ìš©
                if (current_hw.gpu_memory_gb == cached_hw.gpu_memory_gb and 
                    current_hw.cpu_cores == cached_hw.cpu_cores):
                    print("[AutoConfig] ìºì‹œëœ í•˜ë“œì›¨ì–´ ì •ë³´ ì‚¬ìš©")
                    return cached_hw
            except Exception as e:
                print(f"[AutoConfig] ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("[AutoConfig] í•˜ë“œì›¨ì–´ ì •ë³´ ìƒˆë¡œ ê°ì§€")
        return HardwareProfiler.detect_hardware()
    
    def generate_optimal_config(self, user_overrides: Dict[str, Any] = None) -> OptimalSettings:
        """ìµœì í™”ëœ ì„¤ì • ìƒì„±"""
        self.hardware_info = self.load_or_detect_hardware()
        tuner = AutoTuner(self.hardware_info)
        
        # ìë™ ê³„ì‚°ëœ ìµœì ê°’
        optimal_batch_size = tuner.calculate_optimal_batch_size()
        optimal_cpu_workers = tuner.calculate_cpu_workers()
        optimal_queue_size = tuner.calculate_queue_sizes()
        optimal_model = tuner.select_optimal_model()
        optimal_half_precision = tuner.should_use_half_precision()
        
        # ì„±ëŠ¥ ê¸°ë°˜ confidence ì¡°ì •
        if self.hardware_info.gpu_memory_gb >= 8:
            confidence = 0.25  # ê³ ì„±ëŠ¥ GPUëŠ” ë‚®ì€ threshold
        elif self.hardware_info.gpu_memory_gb >= 4:
            confidence = 0.3
        else:
            confidence = 0.4  # ì €ì‚¬ì–‘ GPUëŠ” ë†’ì€ threshold
        
        # Eye detection interval (GPU ì„±ëŠ¥ì— ë”°ë¼)
        if self.hardware_info.gpu_memory_gb >= 6:
            eye_interval = 1  # ëª¨ë“  í”„ë ˆì„
        elif self.hardware_info.gpu_memory_gb >= 4:
            eye_interval = 2  # 2í”„ë ˆì„ë§ˆë‹¤
        else:
            eye_interval = 4  # 4í”„ë ˆì„ë§ˆë‹¤
        
        self.optimal_settings = OptimalSettings(
            batch_size=optimal_batch_size,
            cpu_workers=optimal_cpu_workers,
            queue_size=optimal_queue_size,
            confidence=confidence,
            pose_model=optimal_model,
            eye_detection_interval=eye_interval,
            half_precision=optimal_half_precision,
            opencv_threads=min(optimal_cpu_workers, 8)
        )
        
        # ì‚¬ìš©ì ì˜¤ë²„ë¼ì´ë“œ ì ìš©
        if user_overrides:
            for key, value in user_overrides.items():
                if hasattr(self.optimal_settings, key):
                    setattr(self.optimal_settings, key, value)
                    print(f"[AutoConfig] ì‚¬ìš©ì ì„¤ì • ì ìš©: {key}={value}")
        
        # ì„¤ì • ìºì‹œ ì €ì¥
        self._save_cache()
        
        # ì„¤ì • ì¶œë ¥
        self._print_settings()
        
        return self.optimal_settings
    
    def _save_cache(self):
        """ì„¤ì • ìºì‹œ ì €ì¥"""
        try:
            cache_data = {
                'hardware_info': self.hardware_info.__dict__,
                'optimal_settings': self.optimal_settings.__dict__,
                'timestamp': time.time()
            }
            with open(self.cache_file, 'w') as f:
                json.dump(cache_data, f, indent=2)
        except Exception as e:
            print(f"[AutoConfig] ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _print_settings(self):
        """ìµœì í™”ëœ ì„¤ì • ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸš€ ìë™ ìµœì í™”ëœ ì„¤ì •")
        print("="*50)
        print(f"ğŸ’» CPU: {self.hardware_info.cpu_cores}ì½”ì–´ â†’ {self.optimal_settings.cpu_workers}ì›Œì»¤")
        print(f"ğŸ® GPU: {self.hardware_info.gpu_name}")
        print(f"   â””â”€ ë©”ëª¨ë¦¬: {self.hardware_info.gpu_memory_gb:.1f}GB â†’ ë°°ì¹˜í¬ê¸°: {self.optimal_settings.batch_size}")
        print(f"   â””â”€ ëª¨ë¸: {self.optimal_settings.pose_model}")
        print(f"   â””â”€ Half Precision: {self.optimal_settings.half_precision}")
        print(f"ğŸ”§ ì„±ëŠ¥ ì„¤ì •:")
        print(f"   â””â”€ Confidence: {self.optimal_settings.confidence}")
        print(f"   â””â”€ Eye Interval: {self.optimal_settings.eye_detection_interval}")
        print(f"   â””â”€ Queue Size: {self.optimal_settings.queue_size}")
        print("="*50)
    
    def adapt_runtime_settings(self, current_fps: float, memory_allocated: float):
        """ëŸ°íƒ€ì„ ì„¤ì • ì ì‘í˜• ì¡°ì •"""
        self.runtime_optimizer.record_performance(current_fps, memory_allocated)
        
        # 5ì´ˆë§ˆë‹¤ í•œë²ˆì”©ë§Œ ì¡°ì • (ë„ˆë¬´ ë¹ˆë²ˆí•œ ì¡°ì • ë°©ì§€)
        current_time = time.time()
        if current_time - self.runtime_optimizer.last_adjustment_time < 5.0:
            return self.optimal_settings
        
        if self.runtime_optimizer.should_reduce_batch_size(current_fps):
            # ë°°ì¹˜ í¬ê¸° 50% ê°ì†Œ
            new_batch_size = max(1, self.optimal_settings.batch_size // 2)
            if new_batch_size != self.optimal_settings.batch_size:
                print(f"[RuntimeOptimizer] ë°°ì¹˜ í¬ê¸° ì¡°ì •: {self.optimal_settings.batch_size} â†’ {new_batch_size}")
                self.optimal_settings.batch_size = new_batch_size
                self.runtime_optimizer.last_adjustment_time = current_time
        
        return self.optimal_settings
    
    def handle_oom(self):
        """OOM ì—ëŸ¬ ë°œìƒì‹œ ì„¤ì • ì¡°ì •"""
        self.runtime_optimizer.handle_oom_error()
        
        # ë°°ì¹˜ í¬ê¸° ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
        new_batch_size = max(1, self.optimal_settings.batch_size // 2)
        print(f"[AutoConfig] OOM ì²˜ë¦¬: ë°°ì¹˜í¬ê¸° {self.optimal_settings.batch_size} â†’ {new_batch_size}")
        self.optimal_settings.batch_size = new_batch_size
        
        return self.optimal_settings

# í¸ì˜ í•¨ìˆ˜
def create_auto_optimized_config(user_overrides: Dict[str, Any] = None) -> OptimalSettings:
    """ìë™ ìµœì í™”ëœ ì„¤ì •ì„ ìƒì„±í•˜ëŠ” í¸ì˜ í•¨ìˆ˜"""
    auto_config = AutoConfig()
    return auto_config.generate_optimal_config(user_overrides)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("ğŸ” í•˜ë“œì›¨ì–´ ì •ë³´ ê°ì§€ ì¤‘...")
    hw_info = HardwareProfiler.detect_hardware()
    
    print("\nğŸ“Š ê°ì§€ëœ í•˜ë“œì›¨ì–´:")
    print(f"CPU: {hw_info.cpu_cores}ì½”ì–´ / {hw_info.cpu_threads}ìŠ¤ë ˆë“œ")
    print(f"RAM: {hw_info.total_ram_gb:.1f}GB (ì‚¬ìš©ê°€ëŠ¥: {hw_info.available_ram_gb:.1f}GB)")
    if hw_info.gpu_available:
        print(f"GPU: {hw_info.gpu_name} ({hw_info.gpu_memory_gb:.1f}GB)")
    else:
        print("GPU: ì—†ìŒ")
    
    print("\nâš™ï¸ ìµœì í™”ëœ ì„¤ì • ìƒì„± ì¤‘...")
    optimal_config = create_auto_optimized_config()