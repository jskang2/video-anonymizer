"""
ìë™ ìµœì í™” íŒŒì´í”„ë¼ì¸ - í•˜ë“œì›¨ì–´ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìµœì  ì„¤ì •ì„ ì°¾ì•„ ì‹¤í–‰
Ultraì™€ Speed íŒŒì´í”„ë¼ì¸ì˜ ì¥ì ì„ ê²°í•©í•˜ê³  ìë™ ìµœì í™” ê¸°ëŠ¥ ì¶”ê°€
"""
from __future__ import annotations
import cv2
import numpy as np
from typing import List, Dict
import threading
import queue
import time
import torch
import os
from concurrent.futures import ThreadPoolExecutor, Future
from .detectors import PoseDetector, FaceEyeDetector
from .roi import elbows_from_keypoints, eyes_from_boxes
from .video_io import VideoReader, VideoWriter
from .gpu_accelerated_ops import GPUAcceleratedOps
from .auto_optimizer import AutoConfig, OptimalSettings

class AutoOptimizedPipeline:
    """í•˜ë“œì›¨ì–´ ìì›ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ìµœì í™”í•˜ëŠ” íŒŒì´í”„ë¼ì¸"""

    def __init__(self, cfg, user_overrides: Dict = None):
        print("[AutoPipeline] ğŸ” í•˜ë“œì›¨ì–´ ë¶„ì„ ë° ìµœì í™” ì¤‘...")
        
        # ìë™ ìµœì í™” ì„¤ì • ìƒì„±
        self.auto_config = AutoConfig()
        self.optimal_settings = self.auto_config.generate_optimal_config(user_overrides)
        
        # ì›ë³¸ ì„¤ì • ê°ì²´ ì—…ë°ì´íŠ¸
        self.cfg = cfg
        self._apply_optimal_settings()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜
        self.frame_count = 0
        self.start_time = None
        self.last_memory_check = 0
        self.performance_samples = []
        
        # GPU ìµœì í™” ì„¤ì •
        gpu_settings = {
            'device': getattr(cfg, 'device', 0),
            'confidence': self.optimal_settings.confidence,
            'batch_size': self.optimal_settings.batch_size,
            'half_precision': self.optimal_settings.half_precision
        }

        # ê²€ì¶œê¸° ì´ˆê¸°í™”
        self.pose_detector = PoseDetector(self.optimal_settings.pose_model, **gpu_settings)
        
        # GPU ê°€ì† ì—°ì‚° ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
        if torch.cuda.is_available():
            try:
                # Device ì •ê·œí™”: ì´ë¯¸ cuda: í¬í•¨ëœ ê²½ìš° ì¤‘ë³µ ë°©ì§€
                device = gpu_settings['device']
                if not str(device).startswith('cuda:') and device != 'cpu':
                    device = f"cuda:{device}"
                self.gpu_ops = GPUAcceleratedOps(device=str(device))
                print(f"[AutoPipeline] âš¡ GPU ê°€ì† ì—°ì‚° í™œì„±í™”")
            except Exception as e:
                print(f"[AutoPipeline] âš ï¸ GPU ê°€ì† ì‹¤íŒ¨, CPU ëª¨ë“œ ì‚¬ìš©: {e}")
                self.gpu_ops = None
        else:
            self.gpu_ops = None

        # ë™ì  í í¬ê¸° ì„¤ì •
        queue_size = self.optimal_settings.queue_size
        self.read_queue = queue.Queue(maxsize=queue_size)
        self.cpu_queue = queue.Queue(maxsize=queue_size)
        self.gpu_queue = queue.Queue(maxsize=queue_size)
        self.writer_queue = queue.Queue(maxsize=queue_size)

        # CPU ì›Œì»¤ í’€
        self.cpu_executor = ThreadPoolExecutor(
            max_workers=self.optimal_settings.cpu_workers, 
            thread_name_prefix='auto_cpu_worker'
        )
        self.stop_event = threading.Event()
        
        # OpenCV ìŠ¤ë ˆë“œ ìµœì í™”
        cv2.setNumThreads(self.optimal_settings.opencv_threads)

        print(f"[AutoPipeline] âœ… ì´ˆê¸°í™” ì™„ë£Œ - {self._get_config_summary()}")

    def _apply_optimal_settings(self):
        """ìµœì í™”ëœ ì„¤ì •ì„ ì›ë³¸ cfg ê°ì²´ì— ì ìš©"""
        self.cfg.batch_size = self.optimal_settings.batch_size
        self.cfg.confidence = self.optimal_settings.confidence
        self.cfg.eye_detection_interval = self.optimal_settings.eye_detection_interval
        
        # ìƒˆë¡œìš´ ì„¤ì • ì†ì„±ë“¤ ì¶”ê°€
        if not hasattr(self.cfg, 'cpu_workers'):
            self.cfg.cpu_workers = self.optimal_settings.cpu_workers
        if not hasattr(self.cfg, 'queue_size'):
            self.cfg.queue_size = self.optimal_settings.queue_size

    def _get_config_summary(self) -> str:
        """ì„¤ì • ìš”ì•½ ë¬¸ìì—´ ìƒì„±"""
        return (f"ë°°ì¹˜:{self.optimal_settings.batch_size}, "
                f"ì›Œì»¤:{self.optimal_settings.cpu_workers}, "
                f"ëª¨ë¸:{self.optimal_settings.pose_model}, "
                f"ëˆˆê°„ê²©:{self.optimal_settings.eye_detection_interval}")

    def _read_frames(self, input_path: str):
        """í”„ë ˆì„ ì½ê¸° (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
        try:
            rd = VideoReader(input_path)
            self.start_time = time.time()
            
            for frame_idx, frame in enumerate(rd):
                if self.stop_event.is_set(): 
                    break
                    
                self.read_queue.put((frame_idx, frame))
                self.frame_count = frame_idx + 1
                
                # ì£¼ê¸°ì  ì„±ëŠ¥ ì²´í¬ ë° ì¡°ì •
                if frame_idx % 100 == 0 and frame_idx > 0:
                    self._check_and_adapt_performance()
                    
        except Exception as e:
            print(f"[Reader] ì˜¤ë¥˜: {e}")
            self.stop_event.set()
        finally:
            self.read_queue.put(None)
            print("[Reader] ì™„ë£Œ")

    def _check_and_adapt_performance(self):
        """ì„±ëŠ¥ ì²´í¬ ë° ëŸ°íƒ€ì„ ì¡°ì •"""
        if self.start_time is None:
            return
            
        current_time = time.time()
        elapsed = current_time - self.start_time
        current_fps = self.frame_count / elapsed if elapsed > 0 else 0
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        gpu_allocated, gpu_cached = self.auto_config.runtime_optimizer.monitor_gpu_memory()
        
        # ëŸ°íƒ€ì„ ì„¤ì • ì¡°ì •
        try:
            adapted_settings = self.auto_config.adapt_runtime_settings(current_fps, gpu_allocated)
            
            # ë°°ì¹˜ í¬ê¸°ê°€ ë³€ê²½ëœ ê²½ìš°
            if adapted_settings.batch_size != self.optimal_settings.batch_size:
                self.optimal_settings = adapted_settings
                self.cfg.batch_size = adapted_settings.batch_size
                print(f"[AutoPipeline] ğŸ”„ ëŸ°íƒ€ì„ ì¡°ì • ì ìš©ë¨")
                
        except Exception as e:
            print(f"[AutoPipeline] ì„±ëŠ¥ ì¡°ì • ì¤‘ ì˜¤ë¥˜: {e}")

    def _cpu_task_wrapper(self, gray_frame: np.ndarray) -> tuple[list, list]:
        """ìŠ¤ë ˆë“œ ì•ˆì „í•œ CPU ì‘ì—… ë˜í¼ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
        try:
            face_eye_detector = FaceEyeDetector(self.cfg.face_cascade, self.cfg.eye_cascade)
            return face_eye_detector.detect(gray_frame)
        except cv2.error as cv_err:
            print(f"[CPU Worker] OpenCV ì˜¤ë¥˜: {cv_err}")
            return ([], [])
        except Exception as e:
            print(f"[CPU Worker] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return ([], [])

    def _dispatch_cpu_tasks(self):
        """CPU ì‘ì—… ë¶„ë°° (ì ì‘í˜• ëˆˆ ê²€ì¶œ ê°„ê²©)"""
        try:
            while not self.stop_event.is_set():
                item = self.read_queue.get()
                if item is None:
                    break
                    
                frame_idx, frame = item
                
                # ë™ì  ëˆˆ ê²€ì¶œ ê°„ê²© ì ìš©
                should_detect_eyes = (frame_idx % self.optimal_settings.eye_detection_interval == 0)
                
                if should_detect_eyes:
                    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    future = self.cpu_executor.submit(self._cpu_task_wrapper, gray_frame)
                else:
                    # ëˆˆ ê²€ì¶œ ê±´ë„ˆë›°ê¸°
                    future = Future()
                    future.set_result(([], []))
                    
                self.cpu_queue.put((frame_idx, frame, future))
                
        except Exception as e:
            print(f"[CPU Dispatcher] ì˜¤ë¥˜: {e}")
            self.stop_event.set()
        finally:
            self.cpu_queue.put(None)
            print("[CPU Dispatcher] ì™„ë£Œ")

    def _collect_cpu_results(self):
        """CPU ê²°ê³¼ ìˆ˜ì§‘"""
        try:
            while not self.stop_event.is_set():
                item = self.cpu_queue.get()
                if item is None:
                    break
                    
                frame_idx, frame, future = item
                
                try:
                    _, eyes = future.result(timeout=5.0)  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
                    eye_rois = eyes_from_boxes(eyes, self.cfg.safety_margin_px)
                except Exception as e:
                    print(f"[CPU Collector] Future ê²°ê³¼ ì˜¤ë¥˜: {e}")
                    eye_rois = []
                    
                self.gpu_queue.put((frame_idx, frame, eye_rois))
                
        except Exception as e:
            print(f"[CPU Collector] ì˜¤ë¥˜: {e}")
            self.stop_event.set()
        finally:
            self.gpu_queue.put(None)
            print("[CPU Collector] ì™„ë£Œ")

    def _process_gpu_batch(self):
        """GPU ë°°ì¹˜ ì²˜ë¦¬ (OOM ì²˜ë¦¬ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ í¬í•¨)"""
        processed_batches = 0
        
        try:
            while not self.stop_event.is_set():
                batch_data = []
                sentinel_received = False
                
                try:
                    # ë°°ì¹˜ ìˆ˜ì§‘
                    item = self.gpu_queue.get(timeout=1)
                    if item is None:
                        sentinel_received = True
                    else:
                        batch_data.append(item)
                        
                        # ë™ì  ë°°ì¹˜ í¬ê¸°ê¹Œì§€ ìˆ˜ì§‘
                        while len(batch_data) < self.optimal_settings.batch_size:
                            try:
                                item = self.gpu_queue.get_nowait()
                                if item is None:
                                    sentinel_received = True
                                    break
                                batch_data.append(item)
                            except queue.Empty:
                                break
                                
                except queue.Empty:
                    continue

                if not batch_data and sentinel_received:
                    break
                    
                if batch_data:
                    try:
                        # GPU ë°°ì¹˜ ì²˜ë¦¬
                        frames = [item[1] for item in batch_data]
                        batch_keypoints = self.pose_detector.infer_batch(frames)

                        for i, (frame_idx, frame, eye_rois) in enumerate(batch_data):
                            kpts = batch_keypoints[i][0] if i < len(batch_keypoints) and batch_keypoints[i] else []
                            elbow_rois = elbows_from_keypoints(kpts, self.cfg.safety_margin_px)
                            all_rois = eye_rois + elbow_rois
                            
                            # GPU ê°€ì† ì²˜ë¦¬ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
                            if self.gpu_ops:
                                try:
                                    mask = self.gpu_ops.draw_mask_gpu(frame.shape[:2], all_rois)
                                    out_frame = self.gpu_ops.apply_anonymize_gpu(frame, mask, style=self.cfg.style)
                                except Exception as gpu_err:
                                    print(f"[GPU] GPU ì²˜ë¦¬ ì‹¤íŒ¨, CPU í´ë°±: {gpu_err}")
                                    # CPU í´ë°±
                                    from .roi import draw_soft_mask, apply_anonymize
                                    mask = draw_soft_mask(frame.shape[:2], all_rois)
                                    out_frame = apply_anonymize(frame, mask, style=self.cfg.style)
                            else:
                                # CPU ì²˜ë¦¬
                                from .roi import draw_soft_mask, apply_anonymize
                                mask = draw_soft_mask(frame.shape[:2], all_rois)
                                out_frame = apply_anonymize(frame, mask, style=self.cfg.style)
                                
                            self.writer_queue.put((frame_idx, out_frame))

                        processed_batches += 1
                        
                        # ì£¼ê¸°ì  GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                        if processed_batches % 10 == 0 and torch.cuda.is_available():
                            torch.cuda.empty_cache()
                            
                    except torch.cuda.OutOfMemoryError:
                        # OOM ì²˜ë¦¬
                        print("[GPU] ğŸš¨ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ê°ì§€!")
                        self.auto_config.handle_oom()
                        self.optimal_settings = self.auto_config.optimal_settings
                        
                        # í˜„ì¬ ë°°ì¹˜ ê±´ë„ˆë›°ê¸°
                        for frame_idx, frame, _ in batch_data:
                            # CPU í´ë°± ì²˜ë¦¬
                            from .roi import draw_soft_mask, apply_anonymize
                            mask = draw_soft_mask(frame.shape[:2], [])
                            out_frame = apply_anonymize(frame, mask, style=self.cfg.style)
                            self.writer_queue.put((frame_idx, out_frame))
                            
                if sentinel_received:
                    break
                    
        except Exception as e:
            print(f"[GPU Processor] ì˜¤ë¥˜: {e}")
            self.stop_event.set()
        finally:
            self.writer_queue.put(None)
            print("[GPU Processor] ì™„ë£Œ")

    def _write_frames(self, output_path: str, total_frames: int, w: int, h: int, fps: float):
        """í”„ë ˆì„ ì“°ê¸° (ë²„í¼ í¬ê¸° ì œí•œ í¬í•¨)"""
        wr = VideoWriter(output_path, w, h, fps)
        buffer = {}
        next_idx = 0
        processed_count = 0
        max_buffer_size = min(100, self.optimal_settings.queue_size * 2)  # ë²„í¼ í¬ê¸° ì œí•œ
        
        try:
            while True:
                item = self.writer_queue.get()
                if item is None: 
                    break
                
                frame_idx, frame = item
                buffer[frame_idx] = frame

                # ë²„í¼ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                if len(buffer) > max_buffer_size:
                    # ê°€ì¥ ì˜¤ë˜ëœ í”„ë ˆì„ ë“œë¡­
                    oldest_key = min(buffer.keys())
                    dropped_frame = buffer.pop(oldest_key)
                    print(f"[Writer] âš ï¸ ë²„í¼ ì˜¤ë²„í”Œë¡œìš°, í”„ë ˆì„ {oldest_key} ë“œë¡­")

                # ìˆœì°¨ì  í”„ë ˆì„ ì“°ê¸°
                while next_idx in buffer:
                    out_frame = buffer.pop(next_idx)
                    wr.write(out_frame)
                    processed_count += 1
                    
                    if processed_count % 100 == 0:
                        elapsed = time.time() - self.start_time if self.start_time else 1
                        current_fps = processed_count / elapsed
                        print(f"[Writer] ì§„í–‰ë¥ : {processed_count}/{total_frames} ({current_fps:.1f} FPS)")
                        
                    next_idx += 1
                    
        except Exception as e:
            print(f"[Writer] ì˜¤ë¥˜: {e}")
        finally:
            # ë‚¨ì€ í”„ë ˆì„ ì²˜ë¦¬
            for idx in sorted(buffer.keys()):
                wr.write(buffer[idx])
                processed_count += 1
                
            print(f"[Writer] ìµœì¢… ì™„ë£Œ: {processed_count} í”„ë ˆì„")
            if wr: 
                wr.release()

    def run(self, input_path: str, output_path: str):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        # ì…ë ¥ ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸
        rd = VideoReader(input_path)
        total_frames, w, h, fps = rd.n, rd.w, rd.h, rd.fps
        rd.cap.release()

        if total_frames <= 0: 
            print("[AutoPipeline] âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ë¹„ë””ì˜¤")
            return

        print(f"[AutoPipeline] ğŸ¬ ì²˜ë¦¬ ì‹œì‘: {total_frames}í”„ë ˆì„ ({w}x{h} @ {fps:.1f}fps)")
        start_time = time.time()

        # 5ë‹¨ê³„ ë³‘ë ¬ ìŠ¤ë ˆë“œ ì‹¤í–‰
        threads = [
            threading.Thread(target=self._read_frames, args=(input_path,), name="Reader"),
            threading.Thread(target=self._dispatch_cpu_tasks, name="CPU-Dispatcher"),
            threading.Thread(target=self._collect_cpu_results, name="CPU-Collector"),
            threading.Thread(target=self._process_gpu_batch, name="GPU-Processor"),
            threading.Thread(target=self._write_frames, args=(output_path, total_frames, w, h, fps), name="Writer")
        ]

        # ìŠ¤ë ˆë“œ ì‹œì‘
        for t in threads: 
            t.start()
        
        # Writer ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
        threads[-1].join()
        self.stop_event.set()
        
        # ë‚˜ë¨¸ì§€ ìŠ¤ë ˆë“œ ì •ë¦¬
        for t in threads[:-1]:
            t.join()

        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        self.cpu_executor.shutdown(wait=True)
        
        # ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
        total_time = time.time() - start_time
        final_fps = total_frames / total_time if total_time > 0 else 0
        
        print(f"\nğŸ‰ [AutoPipeline] ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"   â±ï¸  ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {final_fps:.2f} FPS")
        print(f"   ğŸ”§ ìµœì¢… ì„¤ì •: {self._get_config_summary()}")
        
        # ì„±ëŠ¥ ê°œì„  ì œì•ˆ
        if final_fps < 30:
            print(f"   ğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ: ë” ë‚®ì€ í•´ìƒë„ë‚˜ confidence ê°’ ì¦ê°€ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")
        elif final_fps > 100:
            print(f"   ğŸ’¡ í’ˆì§ˆ ê°œì„  ì œì•ˆ: ë” í° ëª¨ë¸ì´ë‚˜ confidence ê°’ ê°ì†Œë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”")